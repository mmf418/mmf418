{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj5Eqd1/bb7xANU83M5xoQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmf418/mmf418/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a155FMLnbSbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb72610-bedb-480b-eb91-5d73a0ddea09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit==1.7.0 --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6AsueoWeGE3",
        "outputId": "d3d9fa84-0cf4-4a33-89c3-e9429baf6352"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2023-01-08 03:45:07.590 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "# 以下を「model.py」に書き込み\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "classes_ja = [\"飛行機\", \"自動車\", \"鳥\", \"猫\", \"鹿\", \"犬\", \"カエル\", \"馬\", \"船\", \"トラック\"]\n",
        "classes_en = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "n_class = len(classes_ja)\n",
        "img_size = 32\n",
        "\n",
        "# CNNのモデル\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 256)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def predict(img):\n",
        "    # モデルへの入力\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((img_size, img_size))\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))  # 平均値を0、標準偏差を1に\n",
        "                                ])\n",
        "    img = transform(img)\n",
        "    x = img.reshape(1, 3, img_size, img_size)\n",
        "\n",
        "    # 訓練済みモデル\n",
        "    net = Net()\n",
        "    net.load_state_dict(torch.load(\n",
        "        \"model_cnn.pth\", map_location=torch.device(\"cpu\")\n",
        "        ))\n",
        "    \n",
        "    # 予測\n",
        "    net.eval()\n",
        "    y = net(x)\n",
        "    \n",
        "   # 結果を返す\n",
        "    y_prob = torch.nn.functional.softmax(torch.squeeze(y))  # 確率で表す\n",
        "    sorted_prob, sorted_indices = torch.sort(y_prob, descending=True)  # 降順にソート\n",
        "    return [(classes_ja[idx], classes_en[idx], prob.item()) for idx, prob in zip(sorted_indices, sorted_prob)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHGDVzVreH6q",
        "outputId": "0d94fb38-2cc6-4499-b725-610c813e9f28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# 以下を「app.py」に書き込み\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from model import predict\n",
        "\n",
        "st.set_option(\"deprecation.showfileUploaderEncoding\", False)\n",
        "\n",
        "st.sidebar.title(\"画像認識アプリ\")\n",
        "st.sidebar.write(\"オリジナルの画像認識モデルを使って何の画像かを判定します。\")\n",
        "\n",
        "st.sidebar.write(\"\")\n",
        "\n",
        "img_source = st.sidebar.radio(\"画像のソースを選択してください。\",\n",
        "                              (\"画像をアップロード\", \"カメラで撮影\"))\n",
        "if img_source == \"画像をアップロード\":\n",
        "    img_file = st.sidebar.file_uploader(\"画像を選択してください。\", type=[\"png\", \"jpg\"])\n",
        "elif img_source == \"カメラで撮影\":\n",
        "    img_file = st.camera_input(\"カメラで撮影\")\n",
        "\n",
        "if img_file is not None:\n",
        "    with st.spinner(\"推定中...\"):\n",
        "        img = Image.open(img_file)\n",
        "        st.image(img, caption=\"対象の画像\", width=480)\n",
        "        st.write(\"\")\n",
        "\n",
        "        # 予測\n",
        "        results = predict(img)\n",
        "\n",
        "        # 結果の表示\n",
        "        st.subheader(\"判定結果\")\n",
        "        n_top = 3  # 確率が高い順に3位まで返す\n",
        "        for result in results[:n_top]:\n",
        "            st.write(str(round(result[2]*100, 2)) + \"%の確率で\" + result[0] + \"です。\")\n",
        "\n",
        "        # 円グラフの表示\n",
        "        pie_labels = [result[1] for result in results[:n_top]]\n",
        "        pie_labels.append(\"others\")\n",
        "        pie_probs = [result[2] for result in results[:n_top]]\n",
        "        pie_probs.append(sum([result[2] for result in results[n_top:]]))\n",
        "        fig, ax = plt.subplots()\n",
        "        wedgeprops={\"width\":0.3, \"edgecolor\":\"white\"}\n",
        "        textprops = {\"fontsize\":6}\n",
        "        ax.pie(pie_probs, labels=pie_labels, counterclock=False, startangle=90,\n",
        "               textprops=textprops, autopct=\"%.2f\", wedgeprops=wedgeprops)  # 円グラフ\n",
        "        st.pyplot(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XckBFr1weZHw",
        "outputId": "18550b43-39f0-4289-9236-2eb4443d33ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2JzPGkJp84xpgDrIPmLaFTd0GH8_47bgtjdZcdfc4eZoeAv5o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K736dOBebUk",
        "outputId": "dd6450ff-8b88-4f48-eeee-c3ef80227f3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&  # 「&>/dev/null&」により、出力を非表示にしてバックグランドジョブとして実行"
      ],
      "metadata": {
        "id": "02S98vKoedoO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()  # プロセスの修了\n",
        "url = ngrok.connect(port=\"2000\")  # 接続"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBczRdkmehr6",
        "outputId": "8e7547d3-1743-4f5b-89f8-195bccb5fd67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:ngrok process starting: 4090\n",
            "2023-01-08 03:52:06.333 INFO    pyngrok.process: ngrok process starting: 4090\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "\n",
            "2023-01-08 03:52:06.358 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "\n",
            "2023-01-08 03:52:06.367 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "\n",
            "2023-01-08 03:52:06.379 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "\n",
            "2023-01-08 03:52:06.382 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "\n",
            "2023-01-08 03:52:06.511 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"client session established\" obj=csess id=a747d23a5b56\n",
            "\n",
            "2023-01-08 03:52:06.516 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"client session established\" obj=csess id=a747d23a5b56\n",
            "\n",
            "INFO:pyngrok.process:ngrok process has started: http://127.0.0.1:4040\n",
            "2023-01-08 03:52:06.526 INFO    pyngrok.process: ngrok process has started: http://127.0.0.1:4040\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=start pg=/api/tunnels id=ed6b994c7d530dee\n",
            "\n",
            "2023-01-08 03:52:06.533 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=start pg=/api/tunnels id=ed6b994c7d530dee\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=end pg=/api/tunnels id=ed6b994c7d530dee status=200 dur=652.424µs\n",
            "\n",
            "2023-01-08 03:52:06.540 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=end pg=/api/tunnels id=ed6b994c7d530dee status=200 dur=652.424µs\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=start pg=/api/tunnels id=12ddc4301c5cdf6f\n",
            "\n",
            "2023-01-08 03:52:06.544 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=start pg=/api/tunnels id=12ddc4301c5cdf6f\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=end pg=/api/tunnels id=12ddc4301c5cdf6f status=200 dur=145.082µs\n",
            "\n",
            "2023-01-08 03:52:06.547 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=end pg=/api/tunnels id=12ddc4301c5cdf6f status=200 dur=145.082µs\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=start pg=/api/tunnels id=58a6b4be6f3322f4\n",
            "\n",
            "2023-01-08 03:52:06.550 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=start pg=/api/tunnels id=58a6b4be6f3322f4\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-2000-e9946119-4b32-450a-9ebb-f7483fbff5bd (http)\" addr=http://localhost:2000 url=http://d1da-35-243-147-89.ngrok.io\n",
            "\n",
            "2023-01-08 03:52:06.617 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-2000-e9946119-4b32-450a-9ebb-f7483fbff5bd (http)\" addr=http://localhost:2000 url=http://d1da-35-243-147-89.ngrok.io\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-2000-e9946119-4b32-450a-9ebb-f7483fbff5bd addr=http://localhost:2000 url=https://d1da-35-243-147-89.ngrok.io\n",
            "\n",
            "2023-01-08 03:52:06.623 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-2000-e9946119-4b32-450a-9ebb-f7483fbff5bd addr=http://localhost:2000 url=https://d1da-35-243-147-89.ngrok.io\n",
            "\n",
            "INFO:pyngrok.process:t=2023-01-08T03:52:06+0000 lvl=info msg=end pg=/api/tunnels id=58a6b4be6f3322f4 status=201 dur=81.164377ms\n",
            "\n",
            "2023-01-08 03:52:06.638 INFO    pyngrok.process: t=2023-01-08T03:52:06+0000 lvl=info msg=end pg=/api/tunnels id=58a6b4be6f3322f4 status=201 dur=81.164377ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-emHOLMel7a",
        "outputId": "fc94cbae-13f2-40a7-c29e-17d0433d1eed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://d1da-35-243-147-89.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit\n",
        "import torch\n",
        "import torchvision\n",
        "import PIL\n",
        "import matplotlib\n",
        "\n",
        "print(\"streamlit==\" + streamlit.__version__)\n",
        "print(\"torch==\" + torch.__version__)\n",
        "print(\"torchvision==\" + torchvision.__version__)\n",
        "print(\"Pillow==\" + PIL.__version__)\n",
        "print(\"matplotlib==\" + matplotlib.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H98-S5GXeqmw",
        "outputId": "5caa0cc8-bd3c-4f3d-9253-d03b8ffd1931"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streamlit==1.7.0\n",
            "torch==1.13.0+cu116\n",
            "torchvision==0.14.0+cu116\n",
            "Pillow==7.1.2\n",
            "matplotlib==3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"requirements.txt\", \"w\") as w:\n",
        "    w.write(\"streamlit==1.8.1\\n\")  # Streamlit Cloud上で動作が確認できたバージョン\n",
        "    w.write(\"torch==1.10.0\\n\")  # Cuda対応は要らないのでcu111は記述しない\n",
        "    w.write(\"torchvision==0.11.1\\n\")  # Cuda対応は要らないのでcu111は記述しない\n",
        "    w.write(\"Pillow\\n\")\n",
        "    w.write(\"matplotlib\\n\")"
      ],
      "metadata": {
        "id": "JtR3zLWyetjf"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}